{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HW5: Decision Trees and Ensemble Methods\n\n## Overview\n\nIn this assignment, you will explore one of the most fundamental concepts in machine learning: the **bias-variance tradeoff**. You'll do this through hands-on experimentation with decision trees and ensemble methods, gaining deep insights into how model complexity affects performance and why ensemble methods often outperform individual models.\n\n## What You'll Learn\n\nBy the end of this assignment, you will understand:\n\n1. **The Bias-Variance Tradeoff**: How model complexity affects the balance between underfitting (high bias) and overfitting (high variance)\n2. **Decision Tree Behavior**: How tree depth impacts model performance and generalization\n3. **Bootstrap Sampling**: How resampling techniques can reduce prediction variance\n4. **Ensemble Methods**: Why combining multiple models often works better than using a single model\n5. **Classification Metrics**: How to evaluate and compare models using multiple performance measures\n6. **Regularization**: How to prevent overfitting in ensemble methods\n\n## Why This Matters\n\n### The Central Challenge of Machine Learning\n\nEvery machine learning model faces a fundamental tradeoff:\n- **High Bias (Underfitting)**: Model is too simple, misses important patterns\n- **High Variance (Overfitting)**: Model is too complex, learns noise instead of signal\n\nThis assignment explores this tradeoff using decision trees, which make this concept particularly visible and intuitive.\n\n### Real-World Applications\n\nDecision trees and ensembles are widely used in industry because they:\n- Handle both numerical and categorical data naturally\n- Provide interpretable results (especially individual trees)\n- Often achieve state-of-the-art performance (especially Random Forests and Gradient Boosting)\n- Work well with limited data preprocessing\n\n**Examples**: Credit scoring, medical diagnosis, feature selection, recommendation systems, fraud detection.\n\n### Key Insights You'll Discover\n\n1. **Why Single Models Struggle**: Individual decision trees tend to overfit, especially when deep\n2. **The Power of Averaging**: Combining predictions from multiple models reduces variance\n3. **Bootstrap Magic**: Training on different random samples helps models capture different aspects of the data\n4. **Ensemble Superiority**: Methods like Random Forest consistently outperform single trees\n\n## Learning Objectives\n\n- **Conceptual**: Understand bias-variance tradeoff through visual analysis\n- **Practical**: Compare single decision trees vs ensemble methods  \n- **Analytical**: Analyze the effect of bootstrap sampling on prediction variance\n- **Technical**: Implement classification metrics from scratch\n- **Applied**: Evaluate models using multiple performance measures\n\n## Background Concepts\n\n### Decision Trees\nDecision trees split data recursively based on feature values, creating a tree-like structure of decisions. Each leaf represents a prediction. Trees are intuitive but prone to overfitting.\n\n### Bootstrap Sampling  \nBootstrap sampling creates new training sets by randomly sampling (with replacement) from the original data. This introduces controlled randomness that helps reduce overfitting.\n\n### Ensemble Methods\nInstead of relying on a single model, ensemble methods combine predictions from multiple models:\n- **Random Forest**: Combines many trees trained on bootstrap samples\n- **Gradient Boosting**: Sequentially builds trees that correct previous mistakes\n\n### The Bias-Variance Decomposition\nFor any model, prediction error comes from three sources:\n1. **Bias**: Error from overly simplistic assumptions\n2. **Variance**: Error from sensitivity to small changes in training data  \n3. **Irreducible Error**: Noise in the data itself\n\n**Key Insight**: Ensemble methods primarily reduce variance while maintaining low bias.\n\n## Assignment Structure\n\nThis assignment is designed as a self-guided exploration with four main parts:\n\n1. **Part 1**: Single Decision Tree Analysis (45-60 min)\n2. **Part 2**: Bootstrap Sampling and Variance (60-90 min)  \n3. **Part 3**: Classification Metrics Analysis (45-60 min)\n4. **Part 4**: Regularization in Ensembles (30-45 min)\n\nEach part builds on the previous one, taking you from basic concepts to advanced ensemble techniques.\n\n## Setup and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import validation_curve, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import our utility functions\n",
    "from data_utils import load_assignment_dataset, get_dataset_info\n",
    "from plot_utils import (\n",
    "    plot_complexity_curve,\n",
    "    plot_roc_curves,\n",
    "    plot_confusion_matrices,\n",
    "    plot_bootstrap_variance,\n",
    "    plot_ensemble_size_comparison,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 1: Single Decision Tree Analysis (45-60 min)\n\n### Understanding Decision Trees\n\nDecision trees are one of the most intuitive machine learning algorithms. They work by recursively splitting the data based on feature values, creating a tree-like structure where:\n- **Internal nodes** represent decisions (e.g., \"Is age > 30?\")\n- **Branches** represent outcomes of decisions  \n- **Leaves** represent final predictions\n\n### The Depth-Complexity Relationship\n\nThe **depth** of a decision tree directly controls its complexity:\n\n- **Shallow trees (depth 1-3)**: \n  - Make simple, broad generalizations\n  - High bias, low variance\n  - May underfit (miss important patterns)\n\n- **Deep trees (depth > 10 or unlimited)**:\n  - Can learn very specific patterns\n  - Low bias, high variance  \n  - May overfit (memorize training data)\n\n### What We'll Explore\n\nIn this section, you'll train decision trees with different maximum depths and observe how:\n1. **Training accuracy** changes with depth\n2. **Test accuracy** changes with depth  \n3. The **gap** between training and test accuracy reveals overfitting\n4. **Cross-validation** helps us find the optimal complexity\n\nThis will give you a concrete, visual understanding of the bias-variance tradeoff.\n\nWe'll start by examining how decision tree depth affects the bias-variance tradeoff."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1.1: Load Dataset and Train Trees\n\n**What you're about to do**: Load a binary classification dataset and train decision trees with different maximum depths. This will let you see how tree complexity affects performance.\n\n**Key concept**: The `max_depth` parameter controls how deep the tree can grow. Deeper trees can make more complex decisions but are more prone to overfitting.\n\nFirst, let's load our dataset and understand its characteristics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X_train, X_test, y_train, y_test = load_assignment_dataset(\"classification\")\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "# Get dataset information\n",
    "dataset_info = get_dataset_info(X_train, y_train)\n",
    "print(f\"\\nDataset info: {dataset_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train decision trees with different max_depth values\n",
    "\n",
    "\n",
    "def train_trees_by_depth(X_train, y_train, max_depths):\n",
    "    \"\"\"\n",
    "    Train decision trees with different maximum depths.\n",
    "\n",
    "    Args:\n",
    "        X_train: 2D numpy array of training features, shape (n_samples, n_features)\n",
    "        y_train: 1D numpy array of training labels, shape (n_samples,)\n",
    "        max_depths: List of integers or None values for max_depth parameter\n",
    "                   (None means unlimited depth)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping max_depth values to fitted DecisionTreeClassifier objects\n",
    "              Each tree should use random_state=42 for reproducibility\n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "max_depths = [1, 3, 5, 10, None]  # None means no limit\n",
    "tree_models = train_trees_by_depth(X_train, y_train, max_depths)\n",
    "\n",
    "print(f\"Trained {len(tree_models)} decision trees with depths: {max_depths}\")\n",
    "\n",
    "# Test each model\n",
    "for depth, model in tree_models.items():\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"Depth {depth}: Train Acc = {train_acc:.3f}, Test Acc = {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 1.1**: Based on the accuracy scores above, which tree depth shows signs of overfitting? How can you tell?\n",
    "\n",
    "**Instructions**: Write your answer in the cell below. Explain your reasoning by comparing training vs test accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**: \n",
    "<!-- Replace this entire cell content with your analysis. Discuss:\n",
    "- Which tree depth(s) show high training accuracy but lower test accuracy\n",
    "- What this pattern indicates about overfitting\n",
    "- How the gap between training and test accuracy changes with depth\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1.2: Complexity Curve Visualization\n\n**What you're about to see**: A complexity curve (also called a validation curve) that shows how model performance changes with complexity.\n\n**Why this matters**: This visualization is one of the most important tools for understanding machine learning models. It reveals the relationship between model complexity and performance on both training and validation data.\n\n**What to look for**: Pay attention to how both training and validation performance change as you vary the tree depth. The patterns you observe will help you understand the bias-variance tradeoff.\n\nNow let's create a complexity curve to visualize the bias-variance tradeoff."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complexity curve data\n",
    "\n",
    "\n",
    "def get_complexity_curve(X_train, y_train, param_name, param_range, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Generate validation curves for model complexity analysis using DecisionTreeClassifier.\n",
    "\n",
    "    Args:\n",
    "        X_train: 2D numpy array of training features, shape (n_samples, n_features)\n",
    "        y_train: 1D numpy array of training labels, shape (n_samples,)\n",
    "        param_name: String parameter name to vary (e.g., 'max_depth')\n",
    "        param_range: List of parameter values to test\n",
    "        cv_folds: Integer, number of cross-validation folds (default 5)\n",
    "\n",
    "    Returns:\n",
    "        tuple: Three elements in this exact order:\n",
    "               - param_range: List of parameter values (same as input)\n",
    "               - train_scores: 2D array of shape (len(param_range), cv) with training scores\n",
    "               - validation_scores: 2D array of shape (len(param_range), cv) with validation scores\n",
    "               Use accuracy scoring and DecisionTreeClassifier with random_state=42\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "param_range, train_scores, val_scores = get_complexity_curve(\n",
    "    X_train, y_train, \"max_depth\", [1, 2, 3, 5, 10, None], cv_folds=5\n",
    ")\n",
    "\n",
    "# Plot the complexity curve\n",
    "fig = plot_complexity_curve(\n",
    "    param_range,\n",
    "    train_scores,\n",
    "    val_scores,\n",
    "    param_name=\"Max Depth\",\n",
    "    title=\"Decision Tree Complexity Curve\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 1.2**: \n",
    "- What happens to training accuracy as tree depth increases? \n",
    "- What happens to validation accuracy? \n",
    "- At what point does the model start overfitting?\n",
    "\n",
    "**Instructions**: Write your answer in the cell below. Analyze the complexity curve plot and discuss the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**:\n",
    "<!-- Replace this entire cell content with your analysis. Discuss:\n",
    "- How training accuracy changes with increasing depth\n",
    "- How validation accuracy changes with increasing depth  \n",
    "- The optimal depth and what this tells you about model complexity\n",
    "- The relationship between bias, variance, and tree depth\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1.3: Optimal Depth Selection\n\n**What we're doing**: Finding the optimal tree depth based on cross-validation performance.\n\n**Key insight**: The depth that gives the highest validation score balances bias and variance optimally. This is where the model generalizes best to unseen data.\n\n**Why test on a holdout set**: After selecting our optimal depth using validation, we test on a completely separate test set to get an unbiased estimate of real-world performance.\n\nLet's identify the optimal tree depth and analyze the overfitting pattern."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal depth (highest validation score)\n",
    "val_means = np.mean(val_scores, axis=1)\n",
    "optimal_idx = np.argmax(val_means)\n",
    "optimal_depth = param_range[optimal_idx]\n",
    "optimal_score = val_means[optimal_idx]\n",
    "\n",
    "print(f\"Optimal depth: {optimal_depth}\")\n",
    "print(f\"Validation score at optimal depth: {optimal_score:.3f}\")\n",
    "\n",
    "# Train final model with optimal depth\n",
    "final_model = DecisionTreeClassifier(max_depth=optimal_depth, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_predictions = final_model.predict(X_test)\n",
    "final_test_acc = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Final test accuracy: {final_test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 1.3**: How does the test accuracy of the optimal model compare to the validation accuracy? What does this tell you about generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: [Write your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Bootstrap Sampling and Variance (60-90 min)\n\n### The Problem with Single Models\n\nYou've just seen how individual decision trees face a fundamental dilemma:\n- **Shallow trees**: High bias (miss important patterns)\n- **Deep trees**: High variance (unstable predictions)\n\n### The Bootstrap Solution\n\n**Bootstrap sampling** offers an elegant solution. Instead of training one model, we:\n1. Create many different training sets by sampling with replacement\n2. Train a separate model on each bootstrap sample  \n3. Average the predictions from all models\n\n### Why Bootstrap Sampling Works\n\n**Mathematical insight**: If you have N independent models, each with error variance σ², the ensemble variance is σ²/N. This is why averaging reduces variance!\n\n**Intuitive explanation**: Different bootstrap samples expose the model to different aspects of the data. When we average:\n- **Systematic patterns** (signal) get reinforced\n- **Random fluctuations** (noise) get cancelled out\n\n### What You'll Discover\n\nIn this section, you'll see:\n1. How individual deep trees make unstable predictions\n2. How bootstrap sampling creates prediction diversity  \n3. How averaging bootstrap predictions reduces variance\n4. Why this leads to better generalization performance\n\n### Ensemble Methods Preview\n\nThe techniques you'll explore here are the foundation of powerful ensemble methods:\n- **Random Forest**: Bootstrap sampling + feature randomness\n- **Gradient Boosting**: Sequential correction of prediction errors\n\nNow we'll explore how bootstrap sampling affects prediction variance and how ensemble methods can reduce this variance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2.1: Bootstrap Variance Analysis\n\n**What you're about to do**: Train 50 deep decision trees on different bootstrap samples and visualize how their predictions vary.\n\n**Key concepts**:\n- **Bootstrap sample**: Random sample WITH replacement from training data\n- **Deep trees**: No depth limit (max_depth=None), prone to overfitting\n- **Prediction variance**: How much predictions differ across bootstrap samples\n\n**What to expect**: Different trees will make different predictions for the same test samples. Some samples will have high variance (trees disagree a lot), others low variance (trees mostly agree).\n\n**Why this matters**: High-variance predictions indicate uncertainty. Samples near decision boundaries typically have higher variance.\n\nLet's train multiple deep decision trees on bootstrap samples and visualize prediction variance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bootstrap predictions with 50 deep trees\n",
    "\n",
    "\n",
    "def bootstrap_sample_predictions(X_train, y_train, X_test, n_trees=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Train multiple deep trees on bootstrap samples and return predictions.\n",
    "\n",
    "    Args:\n",
    "        X_train: 2D numpy array of training features, shape (n_samples, n_features)\n",
    "        y_train: 1D numpy array of training labels, shape (n_samples,)\n",
    "        X_test: 2D numpy array of test features, shape (n_test_samples, n_features)\n",
    "        n_trees: Integer, number of bootstrap trees to train (default 50)\n",
    "        random_state: Integer, random seed for reproducible results (default 42)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2D array of shape (n_trees, n_test_samples)\n",
    "                   Each row contains predictions from one bootstrap tree\n",
    "                   Each column contains predictions for one test sample\n",
    "                   Values should be binary predictions (0 or 1)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "predictions_matrix = bootstrap_sample_predictions(\n",
    "    X_train, y_train, X_test, n_trees=50, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Predictions matrix shape: {predictions_matrix.shape}\")\n",
    "print(f\"Each row represents predictions from one bootstrap tree\")\n",
    "print(f\"Each column represents predictions for one test sample\")\n",
    "\n",
    "# Plot bootstrap variance\n",
    "fig = plot_bootstrap_variance(predictions_matrix, sample_indices=list(range(20)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 2.1**: \n",
    "- What does the variance plot tell you about prediction stability? \n",
    "- Which samples have high variance and what might cause this?\n",
    "- How does the distribution of individual predictions compare to the averaged prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: [Write your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2.2: Single Tree vs Averaged Predictions\n\n**What you're comparing**: The performance of one deep tree versus the average of 50 bootstrap trees.\n\n**The hypothesis**: Theory suggests that averaging predictions from multiple models should reduce prediction variance, but let's test this empirically.\n\n**What you're investigating**: \n- How does the averaged prediction performance compare to a single tree?\n- What does this tell us about the effect of ensemble averaging?\n\n**Note**: Each individual tree in the bootstrap ensemble might perform differently, but we're interested in their collective behavior.\n\nLet's compare the performance of a single deep tree against averaged predictions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single deep tree performance\n",
    "single_tree = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_pred = single_tree.predict(X_test)\n",
    "single_acc = accuracy_score(y_test, single_pred)\n",
    "\n",
    "# Averaged predictions (majority vote)\n",
    "averaged_pred = np.round(np.mean(predictions_matrix, axis=0)).astype(int)\n",
    "averaged_acc = accuracy_score(y_test, averaged_pred)\n",
    "\n",
    "print(f\"Single deep tree accuracy: {single_acc:.3f}\")\n",
    "print(f\"Bootstrap averaged accuracy: {averaged_acc:.3f}\")\n",
    "print(f\"Improvement: {averaged_acc - single_acc:.3f}\")\n",
    "\n",
    "# Calculate variance reduction\n",
    "individual_variance = np.mean(np.var(predictions_matrix, axis=0))\n",
    "print(f\"Average prediction variance across samples: {individual_variance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 2.2**: Why does averaging bootstrap predictions typically improve performance? What is the relationship between variance and accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: [Write your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2.3: Random Forest vs Gradient Boosting\n\n**What you're about to explore**: Two fundamentally different ensemble approaches.\n\n**Random Forest**:\n- Trains trees **in parallel** on bootstrap samples\n- Each tree is independent  \n- Final prediction = **average** of all trees\n- **Reduces variance** while maintaining low bias\n\n**Gradient Boosting**:\n- Trains trees **sequentially**\n- Each new tree corrects mistakes of previous trees\n- Final prediction = **weighted sum** of all trees  \n- **Reduces bias** by iteratively improving fit\n\n**Key difference**: Random Forest focuses on variance reduction through averaging, while Gradient Boosting focuses on bias reduction through sequential improvement.\n\nNow let's implement and compare Random Forest with different numbers of estimators, plus Gradient Boosting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble models\n",
    "\n",
    "\n",
    "def train_ensemble_models(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Train Random Forest and Gradient Boosting models with default parameters.\n",
    "\n",
    "    Args:\n",
    "        X_train: 2D numpy array of training features, shape (n_samples, n_features)\n",
    "        y_train: 1D numpy array of training labels, shape (n_samples,)\n",
    "        random_state: Integer, random seed for reproducible results (default 42)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with exactly these string keys:\n",
    "              - 'Random Forest': fitted RandomForestClassifier object\n",
    "              - 'Gradient Boosting': fitted GradientBoostingClassifier object\n",
    "              Both models should use default sklearn parameters with the given random_state\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "ensemble_models = train_ensemble_models(X_train, y_train, random_state=42)\n",
    "\n",
    "print(\"Ensemble Models Trained:\")\n",
    "for name, model in ensemble_models.items():\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"{name}: Train Acc = {train_acc:.3f}, Test Acc = {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2.4: Ensemble Size Analysis\n\n**What you're investigating**: How the number of trees in a Random Forest affects performance.\n\n**The research question**: Is there an optimal number of trees, or does performance keep improving indefinitely with more trees?\n\n**What to observe**:\n- How do training and test performance change with ensemble size?\n- At what point (if any) do you see diminishing returns?\n- What are the practical implications for choosing ensemble size?\n\nLet's analyze how the number of estimators affects Random Forest performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ensemble sizes\n",
    "\n",
    "\n",
    "def compare_ensemble_sizes(X_train, y_train, X_test, y_test, n_estimators_list):\n",
    "    \"\"\"\n",
    "    Compare Random Forest performance with different numbers of estimators.\n",
    "\n",
    "    Args:\n",
    "        X_train: 2D numpy array of training features, shape (n_samples, n_features)\n",
    "        y_train: 1D numpy array of training labels, shape (n_samples,)\n",
    "        X_test: 2D numpy array of test features, shape (n_test_samples, n_features)\n",
    "        y_test: 1D numpy array of test labels, shape (n_test_samples,)\n",
    "        n_estimators_list: List of integers, number of estimators to test\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with exactly these keys:\n",
    "              - 'n_estimators': list of integers (same as input n_estimators_list)\n",
    "              - 'train_scores': list of floats, accuracy scores on training data\n",
    "              - 'test_scores': list of floats, accuracy scores on test data\n",
    "              Use RandomForestClassifier with random_state=42 for each test\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "estimator_counts = [10, 50, 100]\n",
    "results = compare_ensemble_sizes(X_train, y_train, X_test, y_test, estimator_counts)\n",
    "\n",
    "train_scores = results[\"train_scores\"]\n",
    "test_scores = results[\"test_scores\"]\n",
    "\n",
    "# Plot ensemble size comparison\n",
    "fig = plot_ensemble_size_comparison(estimator_counts, train_scores, test_scores)\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "for i, n in enumerate(estimator_counts):\n",
    "    print(f\"n_estimators={n}: Train={train_scores[i]:.3f}, Test={test_scores[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 2.3**: \n",
    "- How does Random Forest performance change with more estimators?\n",
    "- At what point do diminishing returns set in?\n",
    "- How does Gradient Boosting compare to Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: [Write your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 3: Classification Metrics Analysis (45-60 min)\n\n### Beyond Accuracy: Why Multiple Metrics Matter\n\nSo far, we've used accuracy to evaluate models. But accuracy can be misleading, especially with:\n- **Imbalanced datasets**: When one class is much more common\n- **Cost-sensitive applications**: When different types of errors have different costs\n- **Probability-based decisions**: When you need confidence estimates, not just predictions\n\n### The Classification Metrics Toolbox\n\n**Confusion Matrix**: The foundation - shows all combinations of actual vs predicted labels\n```\n                 Predicted\n                 0    1\nActual    0     TN   FP\n          1     FN   TP\n```\n\n**Core Metrics**:\n- **Accuracy**: (TP + TN) / (TP + TN + FP + FN) - Overall correctness\n- **Precision**: TP / (TP + FP) - Of positive predictions, how many were correct?\n- **Recall**: TP / (TP + FN) - Of actual positives, how many did we catch?\n- **F1-Score**: Harmonic mean of precision and recall\n\n**ROC Analysis**:\n- **ROC Curve**: Shows tradeoff between true positive rate and false positive rate\n- **AUC**: Area Under Curve - single number summarizing ROC performance\n\n### When to Use Each Metric\n\n- **Accuracy**: Balanced datasets, equal error costs\n- **Precision**: When false positives are expensive (e.g., spam detection)\n- **Recall**: When false negatives are expensive (e.g., medical diagnosis)  \n- **F1-Score**: When you want to balance precision and recall\n- **AUC**: When you need a threshold-independent measure\n\n### What You'll Implement\n\nYou'll implement these metrics **from scratch** to understand exactly how they work. This deep understanding will help you:\n1. Choose appropriate metrics for different problems\n2. Interpret results correctly\n3. Debug model performance issues\n\nNow we'll perform a comprehensive evaluation using multiple classification metrics."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3.1: Comprehensive Metrics Calculation\n\n**What you're about to implement**: The four core classification metrics from scratch.\n\n**Why implement from scratch**: \n- Understand exactly what each metric measures\n- See how they relate to the confusion matrix\n- Gain intuition about when each metric is appropriate\n\n**Implementation strategy**:\n1. Start with confusion matrix elements (TP, TN, FP, FN)\n2. Build metrics from these basic building blocks\n3. Handle edge cases (e.g., division by zero)\n\n**Expected insights**: Different models excel at different metrics. A model might have high accuracy but low precision, or high recall but low F1-score.\n\nLet's calculate precision, recall, and F1-score for all our models."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all models for comparison\n",
    "all_models = {\n",
    "    \"Single Tree (Optimal)\": final_model,\n",
    "    \"Single Tree (Deep)\": single_tree,\n",
    "    **ensemble_models,\n",
    "}\n",
    "\n",
    "# =====================================\n",
    "# TODO: IMPLEMENT METRICS FUNCTIONS\n",
    "# =====================================\n",
    "\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for binary classification.\n",
    "\n",
    "    Formula: accuracy = (correct predictions) / (total predictions)\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_pred: 1D numpy array of predicted binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Your code here - implement from scratch without using sklearn\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate precision for binary classification.\n",
    "\n",
    "    Formula: precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_pred: 1D numpy array of predicted binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        float: Precision score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Your code here - implement from scratch without using sklearn\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate recall for binary classification.\n",
    "\n",
    "    Formula: recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_pred: 1D numpy array of predicted binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        float: Recall score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Your code here - implement from scratch without using sklearn\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate F1-score for binary classification.\n",
    "\n",
    "    Formula: f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_pred: 1D numpy array of predicted binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        float: F1 score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    # Your code here - you can use the functions you implemented above\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate all classification metrics at once.\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_pred: 1D numpy array of predicted binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with keys 'accuracy', 'precision', 'recall', 'f1_score'\n",
    "    \"\"\"\n",
    "    # Your code here - use the functions you implemented above\n",
    "    pass\n",
    "\n",
    "\n",
    "# Calculate metrics for each model\n",
    "model_metrics = {}\n",
    "for name, model in all_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = calculate_classification_metrics(y_test, y_pred)\n",
    "    model_metrics[name] = metrics\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3.2: ROC Curve Comparison\n\n**What you're about to implement**: ROC (Receiver Operating Characteristic) curves and AUC calculation.\n\n**Why ROC curves matter**: \n- **Threshold-independent**: Shows performance across all possible decision thresholds\n- **Visual interpretation**: Easy to compare multiple models at a glance\n- **AUC summary**: Single number (0-1) where 1.0 = perfect, 0.5 = random\n\n**How ROC works**:\n1. For each possible threshold, calculate TPR and FPR\n2. Plot TPR vs FPR to create the ROC curve\n3. Calculate AUC using the trapezoidal rule\n\n**Implementation challenge**: You'll need to:\n- Sort predictions by probability\n- Calculate TPR/FPR at each threshold  \n- Compute AUC from the resulting curve\n\nLet's generate ROC curves comparing single tree vs ensemble methods."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC data for key models\n",
    "roc_models = {\n",
    "    \"Single Tree (Deep)\": single_tree,\n",
    "    \"Random Forest\": ensemble_models[\"Random Forest\"],\n",
    "    \"Gradient Boosting\": ensemble_models[\"Gradient Boosting\"],\n",
    "}\n",
    "\n",
    "# =====================================\n",
    "# TODO: IMPLEMENT ROC DATA FUNCTION\n",
    "# =====================================\n",
    "\n",
    "\n",
    "def get_roc_data(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Calculate ROC curve data (FPR, TPR, AUC).\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_proba: 1D numpy array of predicted probabilities for positive class (0.0 to 1.0)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fpr, tpr, auc_score)\n",
    "               - fpr: 1D numpy array of false positive rates\n",
    "               - tpr: 1D numpy array of true positive rates\n",
    "               - auc_score: float, area under ROC curve\n",
    "    \"\"\"\n",
    "    # Your code here - implement from scratch\n",
    "    # Hint: You'll need to:\n",
    "    # 1. Sort predictions and get different threshold values\n",
    "    # 2. For each threshold, calculate TPR and FPR\n",
    "    # 3. Calculate AUC using the trapezoidal rule\n",
    "    pass\n",
    "\n",
    "\n",
    "roc_data = {}\n",
    "for name, model in roc_models.items():\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "    fpr, tpr, auc_score = get_roc_data(y_test, y_proba)\n",
    "    roc_data[name] = (fpr, tpr, auc_score)\n",
    "\n",
    "# Plot ROC curves\n",
    "fig = plot_roc_curves(roc_data, title=\"ROC Curves: Single Tree vs Ensembles\")\n",
    "plt.show()\n",
    "\n",
    "# Print AUC scores\n",
    "print(\"AUC Scores:\")\n",
    "for name, (_, _, auc) in roc_data.items():\n",
    "    print(f\"{name}: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3.3: Confusion Matrix Analysis\n\n**What you're about to implement**: Confusion matrices from scratch.\n\n**Why confusion matrices are essential**:\n- **Complete picture**: Shows all four types of predictions (TP, TN, FP, FN)\n- **Error pattern analysis**: Can reveal if models have systematic biases or preferences\n- **Foundation for other metrics**: All other metrics derive from the confusion matrix\n\n**Implementation approach**: Count occurrences of each (actual, predicted) pair.\n\n**Analysis questions**: Once you generate the matrices, consider what patterns emerge and how different models compare in their error distributions.\n\nLet's create confusion matrices to understand error patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices for key models\n",
    "cm_models = {\n",
    "    \"Single Tree\": single_tree,\n",
    "    \"Random Forest\": ensemble_models[\"Random Forest\"],\n",
    "    \"Gradient Boosting\": ensemble_models[\"Gradient Boosting\"],\n",
    "}\n",
    "\n",
    "# =====================================\n",
    "# TODO: IMPLEMENT CONFUSION MATRIX FUNCTION\n",
    "# =====================================\n",
    "\n",
    "\n",
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Generate confusion matrix for binary classification.\n",
    "\n",
    "    Args:\n",
    "        y_true: 1D numpy array of true binary labels (0 or 1)\n",
    "        y_pred: 1D numpy array of predicted binary labels (0 or 1)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 2x2 confusion matrix with shape (2, 2)\n",
    "                    Format: [[true_negatives, false_positives],\n",
    "                            [false_negatives, true_positives]]\n",
    "    \"\"\"\n",
    "    # Your code here - implement from scratch\n",
    "    # Hint: Count occurrences of each combination of true/predicted labels\n",
    "    pass\n",
    "\n",
    "\n",
    "confusion_matrices = {}\n",
    "for name, model in cm_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = get_confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices[name] = cm\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig = plot_confusion_matrices(confusion_matrices, figsize=(15, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 3.1**: \n",
    "- Which model has the best balance of precision and recall?\n",
    "- What do the confusion matrices reveal about each model's error patterns?\n",
    "- How do the AUC scores relate to the accuracy scores?\n",
    "\n",
    "**Instructions**: Write your analysis in the cell below. Compare the models across different metrics and explain what each metric tells you about model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**:\n",
    "<!-- Replace this entire cell content with your analysis. Discuss:\n",
    "- Which model achieves the best precision/recall balance and why\n",
    "- What patterns you observe in the confusion matrices (false positives vs false negatives)\n",
    "- How AUC scores compare to accuracy and what this reveals about model ranking\n",
    "- Which metric would be most important for a real-world application\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3.4: Model Recommendation\n\n**What you're about to do**: Synthesize all your metrics analysis to make an informed model recommendation.\n\n**The challenge**: Different metrics might favor different models. Your job is to:\n1. Compare models across all metrics  \n2. Consider the tradeoffs between different types of errors\n3. Make a recommendation based on the complete picture\n\n**Questions to consider**:\n- Which model consistently performs well across metrics?\n- Are there any models that excel in specific areas?\n- How do the metrics relate to each other?\n- What would matter most in a real-world application?\n\n**Real-world insight**: Model selection rarely depends on a single metric. The \"best\" model depends on your specific requirements and constraints.\n\nLet's perform a final comparison and make a recommendation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models using our utility function\n",
    "def compare_model_metrics(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compare classification metrics across multiple models.\n",
    "\n",
    "    Args:\n",
    "        models: Dictionary mapping model names to fitted estimators\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "\n",
    "    Returns:\n",
    "        dict: Nested dictionary with model names and their metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Use the metrics functions you implemented above\n",
    "        metrics = calculate_classification_metrics(y_test, y_pred)\n",
    "\n",
    "        # Add AUC using the ROC function you implemented\n",
    "        _, _, auc_score = get_roc_data(y_test, y_proba)\n",
    "        metrics[\"auc\"] = auc_score\n",
    "\n",
    "        results[name] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "final_comparison = compare_model_metrics(all_models, X_test, y_test)\n",
    "\n",
    "print(\"Final Model Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "for model_name, metrics in final_comparison.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Find best model by accuracy\n",
    "best_model = max(final_comparison.keys(), key=lambda x: final_comparison[x][\"accuracy\"])\n",
    "print(f\"Best Model by Accuracy: {best_model}\")\n",
    "print(f\"Accuracy: {final_comparison[best_model]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 3.2**: Based on all the analysis above, justify why you would or would not choose the recommended model for a real-world application. Consider factors like interpretability, computational cost, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: [Write your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 4: Regularization in Ensembles (30-45 min)\n\n### The Regularization Imperative\n\nEven ensemble methods can overfit! While Random Forest reduces variance through averaging, individual trees can still be too complex. **Regularization** helps by constraining model complexity.\n\n### Regularization Strategies in Tree Ensembles\n\n**Parameter-based regularization**:\n- **max_depth**: Limits how deep each tree can grow\n- **min_samples_leaf**: Requires minimum samples in each leaf\n- **min_samples_split**: Requires minimum samples to make a split\n- **max_features**: Limits features considered for each split\n\n### The Overfitting Detection Framework\n\n**Training vs Test Gap**: The key indicator of overfitting\n- **Small gap**: Model generalizes well\n- **Large gap**: Model memorizes training data\n\n**What you'll observe**:\n- Regularized models: Lower training accuracy, better test accuracy\n- Over-regularized models: Poor performance on both training and test\n- Under-regularized models: High training accuracy, poor test accuracy\n\n### Regularization vs Performance Tradeoff\n\nThere's always a tension:\n- **More regularization**: Better generalization, potentially lower peak performance  \n- **Less regularization**: Higher peak performance, risk of overfitting\n\n**The goal**: Find the \"sweet spot\" where the model is complex enough to capture important patterns but simple enough to generalize.\n\n### What You'll Explore\n\n1. **max_depth regularization**: How depth limits affect bias-variance tradeoff\n2. **min_samples_leaf regularization**: How leaf size requirements prevent overfitting\n3. **Overfitting analysis**: Measuring the training-test gap\n4. **Optimal regularization**: Finding the best hyperparameters\n\nFinally, let's explore how regularization parameters affect ensemble performance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 4.1: Random Forest Regularization\n\n**What you're about to explore**: How regularization parameters affect Random Forest performance.\n\n**Two key parameters**:\n\n1. **max_depth**: Controls tree complexity directly\n   - Lower values → simpler trees, higher bias, lower variance\n   - Higher values → complex trees, lower bias, higher variance\n\n2. **min_samples_leaf**: Controls leaf size requirements  \n   - Higher values → fewer, larger leaves, smoother decision boundaries\n   - Lower values → more, smaller leaves, more detailed decision boundaries\n\n**What to expect**:\n- **Default Random Forest**: Often performs well without tuning\n- **Over-regularized**: Both training and test accuracy suffer\n- **Under-regularized**: High training accuracy, lower test accuracy\n- **Sweet spot**: Good balance of training and test performance\n\nLet's explore different regularization parameters for Random Forest."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_depth values for Random Forest\n",
    "rf_depths = [3, 5, 10, None]\n",
    "rf_depth_results = []\n",
    "\n",
    "for depth in rf_depths:\n",
    "    rf = RandomForestClassifier(n_estimators=50, max_depth=depth, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, rf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "    rf_depth_results.append((depth, train_acc, test_acc))\n",
    "    print(f\"RF max_depth={depth}: Train={train_acc:.3f}, Test={test_acc:.3f}\")\n",
    "\n",
    "# Test different min_samples_leaf values\n",
    "rf_min_samples = [1, 5, 10, 20]\n",
    "rf_samples_results = []\n",
    "\n",
    "for min_samples in rf_min_samples:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=50, min_samples_leaf=min_samples, random_state=42\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, rf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "    rf_samples_results.append((min_samples, train_acc, test_acc))\n",
    "    print(\n",
    "        f\"RF min_samples_leaf={min_samples}: Train={train_acc:.3f}, Test={test_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 4.2: Regularization Learning Curves\n\n**What you're visualizing**: How regularization affects the bias-variance tradeoff.\n\n**Investigation focus**: Look for patterns in how training and test performance change as you vary regularization parameters.\n\n**Questions to consider**:\n- How do training and test scores relate to each other across different parameter values?\n- Can you identify regions where the model might be too simple or too complex?\n- What does the gap between training and test performance tell you?\n\nLet's visualize how regularization affects the bias-variance tradeoff."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning curves for regularization parameters\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Max depth regularization\n",
    "depths, train_accs, test_accs = zip(*rf_depth_results)\n",
    "depth_labels = [str(d) if d is not None else \"None\" for d in depths]\n",
    "\n",
    "ax1.plot(range(len(depths)), train_accs, \"o-\", label=\"Training\", color=\"blue\")\n",
    "ax1.plot(range(len(depths)), test_accs, \"o-\", label=\"Test\", color=\"red\")\n",
    "ax1.set_xlabel(\"Max Depth\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_title(\"Random Forest: Max Depth Regularization\")\n",
    "ax1.set_xticks(range(len(depths)))\n",
    "ax1.set_xticklabels(depth_labels)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Min samples leaf regularization\n",
    "min_samples, train_accs, test_accs = zip(*rf_samples_results)\n",
    "\n",
    "ax2.plot(min_samples, train_accs, \"o-\", label=\"Training\", color=\"blue\")\n",
    "ax2.plot(min_samples, test_accs, \"o-\", label=\"Test\", color=\"red\")\n",
    "ax2.set_xlabel(\"Min Samples Leaf\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_title(\"Random Forest: Min Samples Leaf Regularization\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 4.3: Connecting Regularization to Overfitting\n\n**What you're analyzing**: The relationship between regularization strength and the training-test performance gap.\n\n**The key metric**: Training accuracy - Test accuracy\n- This gap can help diagnose overfitting patterns\n- Different regularization values will produce different gaps\n\n**Your investigation**: \n- How does this gap change with different regularization settings?\n- What does the gap tell you about model generalization?\n- Which regularization approach seems most effective at controlling overfitting?\n\nLet's analyze how regularization parameters affect overfitting patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overfitting gap (train_acc - test_acc) for each configuration\n",
    "print(\"Overfitting Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Max Depth Regularization:\")\n",
    "for depth, train_acc, test_acc in rf_depth_results:\n",
    "    gap = train_acc - test_acc\n",
    "    print(\n",
    "        f\"  max_depth={depth}: Gap = {gap:.3f} {'(overfitting)' if gap > 0.05 else '(good fit)'}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nMin Samples Leaf Regularization:\")\n",
    "for min_samples, train_acc, test_acc in rf_samples_results:\n",
    "    gap = train_acc - test_acc\n",
    "    print(\n",
    "        f\"  min_samples_leaf={min_samples}: Gap = {gap:.3f} {'(overfitting)' if gap > 0.05 else '(good fit)'}\"\n",
    "    )\n",
    "\n",
    "# Recommend optimal regularization\n",
    "best_depth_config = min(\n",
    "    rf_depth_results, key=lambda x: abs(x[1] - x[2])\n",
    ")  # Minimize gap\n",
    "best_samples_config = min(rf_samples_results, key=lambda x: abs(x[1] - x[2]))\n",
    "\n",
    "print(f\"\\nRecommended Regularization:\")\n",
    "print(\n",
    "    f\"  max_depth = {best_depth_config[0]} (gap: {best_depth_config[1] - best_depth_config[2]:.3f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  min_samples_leaf = {best_samples_config[0]} (gap: {best_samples_config[1] - best_samples_config[2]:.3f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question 4.1**: \n",
    "- How do regularization parameters affect the training vs test accuracy gap?\n",
    "- Which regularization method seems more effective for this dataset?\n",
    "- What's the tradeoff between regularization and model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: [Write your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary and Final Reflection\n\n### Congratulations! \n\nYou've completed a comprehensive exploration of decision trees, ensemble methods, and the bias-variance tradeoff. Now it's time to reflect on what you've discovered through your analysis.\n\n### Major Concepts You've Explored\n\n1. **Bias-Variance Tradeoff**: The fundamental tension in machine learning between underfitting and overfitting\n\n2. **Decision Tree Complexity**: How tree depth affects model behavior and generalization\n\n3. **Bootstrap Sampling**: How resampling with replacement creates model diversity\n\n4. **Ensemble Methods**: How combining multiple models can improve performance\n\n5. **Classification Metrics**: How to evaluate models using multiple performance measures\n\n6. **Regularization**: How to control model complexity in ensemble methods\n\n### Connecting Theory to Practice\n\nThe techniques you've learned are foundational to modern machine learning:\n\n- **Random Forests** are widely used in industry for their robustness and interpretability\n- **Gradient Boosting** methods (like XGBoost, LightGBM) are popular in machine learning competitions\n- **Bootstrap methods** appear throughout statistics and machine learning\n- **Cross-validation** and **regularization** are essential tools for any practitioner\n\n### Real-World Applications\n\nThese methods excel in:\n- **Tabular data problems**: Where trees naturally handle mixed data types\n- **Feature importance**: Trees provide interpretable feature rankings  \n- **Robust performance**: Ensembles work well across diverse problem types\n- **Limited preprocessing**: Trees handle raw data better than many alternatives\n\n**Final Reflection Questions**:\n\n1. **Bias-Variance Tradeoff**: How did the complexity curves demonstrate the bias-variance tradeoff? What patterns did you observe?\n\n2. **Ensemble Benefits**: What are the main advantages of ensemble methods over single decision trees? When might you prefer a single tree?\n\n3. **Bootstrap Sampling**: How does bootstrap sampling contribute to variance reduction in Random Forest?\n\n4. **Model Selection**: Based on your analysis, what factors should guide model selection in practice?\n\n5. **Regularization**: How does regularization in ensemble methods differ from regularization in single models?\n\n**Your Final Answers**: [Write your comprehensive reflection here]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Tests\n",
    "\n",
    "Run the following cells to verify your implementations are working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Dataset loading\n",
    "assert X_train.shape[0] > 0, \"Training data should not be empty\"\n",
    "assert X_test.shape[0] > 0, \"Test data should not be empty\"\n",
    "assert len(np.unique(y_train)) == 2, \"Should be binary classification\"\n",
    "print(\"✓ Dataset loading test passed\")\n",
    "\n",
    "# Test 2: Tree models\n",
    "assert len(tree_models) == len(max_depths), \"Should have one model per depth\"\n",
    "assert all(\n",
    "    hasattr(model, \"predict\") for model in tree_models.values()\n",
    "), \"All should be trained models\"\n",
    "print(\"✓ Tree training test passed\")\n",
    "\n",
    "# Test 3: Bootstrap predictions\n",
    "assert predictions_matrix.shape == (\n",
    "    50,\n",
    "    len(X_test),\n",
    "), f\"Expected (50, {len(X_test)}), got {predictions_matrix.shape}\"\n",
    "assert np.all(\n",
    "    (predictions_matrix == 0) | (predictions_matrix == 1)\n",
    "), \"Predictions should be 0 or 1\"\n",
    "print(\"✓ Bootstrap predictions test passed\")\n",
    "\n",
    "# Test 4: Ensemble models\n",
    "assert \"Random Forest\" in ensemble_models, \"Should include Random Forest\"\n",
    "assert \"Gradient Boosting\" in ensemble_models, \"Should include Gradient Boosting\"\n",
    "print(\"✓ Ensemble models test passed\")\n",
    "\n",
    "# Test 5: Metrics calculation\n",
    "sample_metrics = list(model_metrics.values())[0]\n",
    "required_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "assert all(\n",
    "    metric in sample_metrics for metric in required_metrics\n",
    "), \"Missing required metrics\"\n",
    "print(\"✓ Metrics calculation test passed\")\n",
    "\n",
    "print(\"\\n🎉 All tests passed! Your implementation is working correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}